{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of Project 5. Fake News Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team SAO presents to you a high accuracy model to predict whether the news is real or fake."
      ],
      "metadata": {
        "id": "_QakZYWA0YB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description**\r\n",
        "\r\n",
        "1. id: Unique serial number of the news\r\n",
        "2. title: Title of a news\r\n",
        "3. author: Author/Editor of the news article\r\n",
        "4. content: The text of the article\r\n",
        "5. label: a label that marks whether the news article is real or fake:\r\n",
        "           1: if Fake news\r\n",
        "           0: if real News\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "FaIBmnXCknPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Dependencies"
      ],
      "metadata": {
        "id": "k399dHafvL5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "outputs": [],
      "metadata": {
        "id": "-fetC5yqkPVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')#downloading stopwords package"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AC1YpmGwIDw",
        "outputId": "deb5c972-ac0a-43c1-f0d5-6cfa30e498fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# Printing stopwords in English\r\n",
        "print(stopwords.words('english'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxIOt3DowpUR",
        "outputId": "9f3409ac-c6bf-4276-8767-cbc9e00f2781"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing"
      ],
      "metadata": {
        "id": "NjeGd1CLw_6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# loading our train.csv dataset to a pandas DataFrame\r\n",
        "news_dataset = pd.read_csv('/content/train.csv')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ba42a1199b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading our train.csv dataset to a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnews_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 4205"
          ]
        }
      ],
      "metadata": {
        "id": "nCGcpu_1wzLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "b9e7eb9c-9b13-4279-8054-90741e8a9298"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "news_dataset.shape"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-45c5752a66f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'news_dataset' is not defined"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "aRgmbYSbxV4-",
        "outputId": "8d4581fc-fec7-4715-fa78-b2b3a1baa61b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# To see starting few data entries\r\n",
        "news_dataset.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jjJ1eB6RxZaS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Here Team SAO is counting the number of missing values in our dataset\r\n",
        "news_dataset.isnull().sum()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QYkDi4SwxlKi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Replacing the null values with empty string to increase the accuracy of dataset\r\n",
        "news_dataset = news_dataset.fillna('')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mc04lQrhx57m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# merging the columns, author name and title present in the dataset\r\n",
        "news_dataset['content'] = news_dataset['author']+' '+news_dataset['title']"
      ],
      "outputs": [],
      "metadata": {
        "id": "H7TZgHszygxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(news_dataset['content'])#lets see what is in the content section"
      ],
      "outputs": [],
      "metadata": {
        "id": "cbF6GBBpzBey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# separating the data & label and assigning varaible X and Y to them\r\n",
        "X = news_dataset.drop(columns='label', axis=1)\r\n",
        "Y = news_dataset['label']"
      ],
      "outputs": [],
      "metadata": {
        "id": "LfBtAvLtzEo6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X)\r\n",
        "print(Y)"
      ],
      "outputs": [],
      "metadata": {
        "id": "oHPBr540zl1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Stemming ?\n",
        "\n",
        "Stemming is a method of reducing a word to its ROOT WORD\n",
        "\n",
        "like:\n",
        "coder, coding, codes --> code ||\n",
        "\n",
        "Let's use this feature"
      ],
      "metadata": {
        "id": "0NwFcpqcz37a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "port_stem = PorterStemmer()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ga_DaZxhzoWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def stemming(content):\r\n",
        "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\r\n",
        "    stemmed_content = stemmed_content.lower()\r\n",
        "    stemmed_content = stemmed_content.split()\r\n",
        "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\r\n",
        "    stemmed_content = ' '.join(stemmed_content)\r\n",
        "    return stemmed_content"
      ],
      "outputs": [],
      "metadata": {
        "id": "zY-n0dCh0e-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Applying stemming on content section of our dataset"
      ],
      "metadata": {
        "id": "rU619EGAtmFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "news_dataset['content'] = news_dataset['content'].apply(stemming)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MBUIk4c94yTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(news_dataset['content']) #to see the effect stemming had on our dataset i.e. all words comes to its root version"
      ],
      "outputs": [],
      "metadata": {
        "id": "xmwK-zyO5Stg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#separating the data and label columns\r\n",
        "X = news_dataset['content'].values\r\n",
        "Y = news_dataset['label'].values"
      ],
      "outputs": [],
      "metadata": {
        "id": "5ZIidnta5k5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X) #to see what went in variable X"
      ],
      "outputs": [],
      "metadata": {
        "id": "3nA_SBZX6BeH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(Y) #to see what went in variable Y"
      ],
      "outputs": [],
      "metadata": {
        "id": "NgkFGXkg6HS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Y.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "Iu2ZEBkL6QTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# converting the textual data(STRING) to numerical data using vectorizer to make the model simpler.\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectorizer.fit(X)\r\n",
        "\r\n",
        "X = vectorizer.transform(X)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BMfepsQZ6TES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MJj5esbs7Nzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset to training & test data\r\n",
        "\r\n",
        "1.   80% in training\r\n",
        "2.   20% in test\r\n",
        "3.   Stratifying Y to ensure all training or test set doesn't get only one kind of outcome\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "mKBRGiSQ7YCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VjMYwmBo7Pbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training our Model: Bringing in Logistic Regression"
      ],
      "metadata": {
        "id": "rxDsQvgO8Oln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = LogisticRegression()"
      ],
      "outputs": [],
      "metadata": {
        "id": "HrSItcqc7qAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.fit(X_train, Y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fdVJ839l8Vgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result time for the Model"
      ],
      "metadata": {
        "id": "sbPKIFT89W1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Accuracy Score**"
      ],
      "metadata": {
        "id": "YG6gqVty9ZDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# accuracy score on the training data\r\n",
        "X_train_prediction = model.predict(X_train)\r\n",
        "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VgwtWZY59PBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Accuracy score of the training data : ', training_data_accuracy*100, \"%\") # printing accuracy score of our training data"
      ],
      "outputs": [],
      "metadata": {
        "id": "4L-r5mld-BFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# accuracy score on the test data\r\n",
        "X_test_prediction = model.predict(X_test)\r\n",
        "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Kgcn13oO-H6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Accuracy score of the test data : ', test_data_accuracy*100, \"%\") # printing accuracy score of our test data"
      ],
      "outputs": [],
      "metadata": {
        "id": "9TG0Yof1-vg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have got a nice score, let's predict now"
      ],
      "metadata": {
        "id": "Yun4seaE-6tV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "k = int(input(\"\\nEnter News Id No. to check the article: \")) #For eg. enter 3 or 4 or any label number from the dataset\r\n",
        "\r\n",
        "X_new = X_test[k]\r\n",
        "\r\n",
        "prediction = model.predict(X_new) #predicting\r\n",
        "print(prediction)\r\n",
        "\r\n",
        "if (prediction[0]==0):\r\n",
        "  print('\\t\\tThe news is Real\\n')\r\n",
        "else:\r\n",
        "  print('\\t\\tThe news is Fake\\n')\r\n",
        "\r\n",
        "print(Y_test[k])  "
      ],
      "outputs": [],
      "metadata": {
        "id": "lPjssDL_-zo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "8KaWdvDI_eUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "JBbWkLGr_lb_"
      }
    }
  ]
}